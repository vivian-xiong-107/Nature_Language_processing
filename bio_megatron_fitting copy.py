# -*- coding: utf-8 -*-
"""Bio_Megatron_fitting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11vki4CtuvooA71IBTIkH5ietBX-KvTAk
"""

BRANCH = 'v1.0.2'
!python -m pip install git+https://github.com/NVIDIA/NeMo.git

# Commented out IPython magic to ensure Python compatibility.
# %%writefile setup.sh
# 
# export CUDA_HOME=/usr/local/cuda-10.1
# git clone https://github.com/NVIDIA/apex
# pip install -v --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./apex

!sh setup.sh

!pip install rapidfuzz
!pip install megatron 
!pip install omegaconf
! pip install nemo_toolkit[all]
from nemo.collections import nlp as nemo_nlp
from nemo.utils.exp_manager import exp_manager

import os
import wget 
import torch
import pytorch_lightning as pl
from omegaconf import OmegaConf

TASK = 'mtsamples'
DATA_DIR = os.path.join(os.getcwd(), 'DATA_DIR')
RE_DATA_DIR = os.path.join(DATA_DIR, 'RE')
WORK_DIR = os.path.join(os.getcwd(), 'WORK_DIR')
MODEL_CONFIG = 'text_classification_config.yaml'

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(os.path.join(DATA_DIR, 'RE'), exist_ok=True)
os.makedirs(WORK_DIR, exist_ok=True)

ABS_PATH ='/content/DATA_DIR/RE/mtsamples.csv'
RE_DATA_DIR = os.path.join(ABS_PATH, 'data')

RE_DATA_DIR ='/content/DATA_DIR/RE'

! ls -l $RE_DATA_DIR

BRANCH = 'v1.0.2'
!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]

"""Token Classification Model requires the data to be split into 2 files:

*   text.txt
*   labels.txt

Each line of the text.txt file contains text sequences, where words are separated with spaces, i.e.: [WORD] [SPACE] [WORD] [SPACE] [WORD].

The labels.txt file contains corresponding labels for each word in text.txt, the labels are separated with spaces, i.e.: [LABEL] [SPACE] [LABEL] [SPACE] [LABEL].


"""

import pandas as pd

df = pd.read_csv('/content/DATA_DIR/RE/mtsamples.csv')

import string

no_punc_translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))
df['transcription_lower']=df['transcription'].apply(lambda x: ' '.join([i for i in str(x).lower().translate(no_punc_translator).split(' ') if i.isalpha()]))

df.head(5)

from sklearn.model_selection import train_test_split

train_df, eval_df = train_test_split(df, test_size=0.2)

train_df.shape

eval_df.shape

text_train = train_df[['transcription_lower']]
text_eval = eval_df[['transcription_lower']]

label_train = train_df[['medical_specialty']]
label_eval = eval_df[['medical_specialty']]

text_train.head(5)

text_train.to_csv(r'/content/DATA_DIR/RE/text_train.txt', header=None, index=None, sep='\t', mode='a')
label_train.to_csv(r'/content/DATA_DIR/RE/labels_train.txt', header=None, index=None, sep='\t', mode='a')

text_eval.to_csv(r'/content/DATA_DIR/RE/text_eval.txt', header=None, index=None, sep='\t', mode='a')
label_eval.to_csv(r'/content/DATA_DIR/RE/labels_eval.txt', header=None, index=None, sep='\t', mode='a')

!head -n 5 {RE_DATA_DIR}/labels_eval.txt

"""### Model Configuration"""

WORK_DIR = "WORK_DIR"
os.makedirs(WORK_DIR, exist_ok=True)
MODEL_CONFIG = "token_classification_config.yaml"

config_dir = WORK_DIR + '/configs/'
os.makedirs(config_dir, exist_ok=True)
if not os.path.exists(config_dir + MODEL_CONFIG):
    print('Downloading config file...')
    wget.download('https://raw.githubusercontent.com/NVIDIA/NeMo/v1.0.0b2/examples/nlp/token_classification/conf/' + MODEL_CONFIG, config_dir)
else:
    print ('config file is already exists')

# this line will print the entire config of the model
config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'
print(config_path)
config = OmegaConf.load(config_path)
# Note: these are small batch-sizes - increase as appropriate to available GPU capacity
config.model.train_ds.batch_size=8
config.model.validation_ds.batch_size=8

print(OmegaConf.to_yaml(config))

"""### Model Training"""

config.model.dataset.data_dir = os.path.join(RE_DATA_DIR, 'NER')

print("Trainer config - \n")
print(OmegaConf.to_yaml(config.trainer))



































