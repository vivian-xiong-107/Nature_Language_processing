# -*- coding: utf-8 -*-
"""data_pretraining_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/181PrlkxK6bdWm43qBWWC7p4_LUQgNXvQ
"""

import numpy as np
from pandas.io.parsers import read_csv
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
import string
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow import keras

clinical_text_df = read_csv("/content/sample_data/mtsamples.csv")

"""### Spliting the sentences within the transcription into words

"""

no_punc_translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))
clinical_text_df['transcription_lower']=clinical_text_df['transcription'].apply(lambda x: ' '.join([i for i in str(x).lower().translate(no_punc_translator).split(' ') if i.isalpha()]))

"""### Spliting the whole datasets into train set and test set"""

x = clinical_text_df['transcription_lower']
y = clinical_text_df['medical_specialty']
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, shuffle=False)

x_train.shape

y_train.shape

x_train.head(5)

y_train.head(20)

clinical_text_df.head(1)

import torch
from tqdm import tqdm
from transformers import BertTokenizer, BertModel

"""### Model setting 


1.   Model Type
2.   Tokenizer Type

1.   Batch Size
2.   Max Size





"""

MODEL_TYPE = ''
MAX_SIZE = ''
BATCH_SIZE = ''

tokenizer = BertTokenizer.from_pretrained()
model = BertModel.from_pretrained()

tokenized_input = clinical_text_df['transcription_lower'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))

print(tokenized_input[1])

print("Here 101 -> [CLS] and 102 -> [SEP]")

padded_tokenized_input = np.array([i + [0]*(MAX_SIZE-len(i)) for i in tokenized_input.values],dtype="object")

print(padded_tokenized_input[1])

attention_masks  = np.where(padded_tokenized_input != 0, 1, 0)

print(attention_masks[1])

input_ids = torch.tensor(padded_tokenized_input)  
attention_masks = torch.tensor(attention_masks)


all_train_embedding = []

with torch.no_grad():
  for i in tqdm(range(0,len(input_ids),200)):    
    last_hidden_states = model(input_ids[i:min(i+200,len(x_train))], attention_mask = attention_masks[i:min(i+200,len(x_train))])[0][:,0,:].numpy()
    all_train_embedding.append(last_hidden_states)


unbatched_train = []
for batch in all_train_embedding:
    for seq in batch:
        unbatched_train.append(seq)

train_labels = x_train['target']

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test =  train_test_split(unbatched_train, train_labels, test_size=0.33, random_state=42, stratify=train_labels)

